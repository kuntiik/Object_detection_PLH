{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vid_4_9800.jpg</th>\n",
       "      <td>18.709395</td>\n",
       "      <td>192.414994</td>\n",
       "      <td>97.97815</td>\n",
       "      <td>231.301004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vid_4_9800.jpg</th>\n",
       "      <td>630.703569</td>\n",
       "      <td>180.109294</td>\n",
       "      <td>676.00000</td>\n",
       "      <td>214.073025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      xmin        ymin       xmax        ymax\n",
       "image                                                        \n",
       "vid_4_9800.jpg   18.709395  192.414994   97.97815  231.301004\n",
       "vid_4_9800.jpg  630.703569  180.109294  676.00000  214.073025"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_annotations = \"/home.stud/kuntluka/dataset/cars/data/train_solution_bounding_boxes.csv\"\n",
    "df = pd.read_csv(cars_annotations, index_col='image')\n",
    "df.head()\n",
    "print(len(df.index.unique().values))\n",
    "# print(len(df.index.values))\n",
    "imgs = df.index.unique().values\n",
    "df.loc[imgs[2]]\n",
    "df.loc['vid_4_9800.jpg']\n",
    "# df.image\n",
    "# df[0.:]\n",
    "\n",
    "# df[0]\n",
    "# ann_file = csv.reader(cars_annotations)\n",
    "# ann_file[0]\n",
    "# df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "class CarsDataset(Dataset):\n",
    "    def __init__(self, img_root, ann_file):\n",
    "        super().__init__()\n",
    "        self.root = Path(img_root)\n",
    "        self.df = pd.read_csv(ann_file, index_col='image')\n",
    "        # self.ids = list(df.index.unique().values)\n",
    "        self.ids = [_.name for _ in self.root.iterdir()]\n",
    "        self.ids_with_box = self.df.index.unique().values\n",
    "        self.transforms = A.Compose([\n",
    "            A.Resize(width=480, height=480),\n",
    "            A.Normalize(mean=(0,0,0), std=(1,1,1)),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'], min_visibility=0.4, min_area=5))\n",
    "        self.num_cat = 1\n",
    "        self.imgs = [Image.open(self.root / p) for p in self.ids]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        if self.ids[index] in self.ids_with_box:\n",
    "            target = self.df.loc[self.ids[index]].values\n",
    "        else:\n",
    "            target = np.array([])\n",
    "\n",
    "        if target.ndim == 1:\n",
    "            target = np.expand_dims(target, axis=0)\n",
    "        n_boxes = np.size(target, 0)\n",
    "        if np.size(target, 1) == 0:\n",
    "            n_boxes = 0\n",
    "            target = []\n",
    "\n",
    "        labels = [0 for _ in range(n_boxes)]\n",
    "        transformed = self.transforms(image=np.array(img), bboxes=target, class_labels=labels)\n",
    "        target = {}\n",
    "        img = transformed['image']\n",
    "        target[\"boxes\"] = torch.tensor(transformed[\"bboxes\"])\n",
    "        target[\"labels\"] = torch.tensor(transformed[\"class_labels\"])\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_mean_std(dataloader : DataLoader):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for data, _ in dataloader:\n",
    "        data = torch.stack(data)\n",
    "        channels_sum += torch.mean(data, dim=[0,2,3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n",
    "        num_batches += 1\n",
    "    \n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_squared_sum/num_batches - mean**2)**0.5\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean : tensor([0.2432, 0.3255, 0.3522]) , std : tensor([0.2186, 0.2592, 0.3083])\n"
     ]
    }
   ],
   "source": [
    "cars_annotations = \"/home.stud/kuntluka/dataset/cars/data/train_solution_bounding_boxes.csv\"\n",
    "training_root = \"/home.stud/kuntluka/dataset/cars/data/training_images\"\n",
    "dataset = CarsDataset(training_root, cars_annotations)\n",
    "# dataset[1]\n",
    "\n",
    "def collate(batch):\n",
    "    return tuple(zip(*batch))\n",
    "dl = DataLoader(dataset, 8, collate_fn=collate)\n",
    "# next(iter(dl))\n",
    "mean, std = get_dataset_mean_std(dl)\n",
    "print(f\"mean : {mean} , std : {std}\")\n",
    "# all_y = {}\n",
    "# for i in range(300):\n",
    "#     x,y = dataset[i]\n",
    "#     all_y[i] = y['labels']\n",
    "\n",
    "        \n",
    "    # if len(y['labels']) == 0:\n",
    "    #     print(y)\n",
    "    # if len(y['labels']) == 2:\n",
    "    #     print(y)\n",
    "    # print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([0, 0]), 1: tensor([]), 2: tensor([0, 0, 0, 0]), 3: tensor([]), 4: tensor([]), 5: tensor([0, 0]), 6: tensor([0]), 7: tensor([]), 8: tensor([0, 0, 0]), 9: tensor([]), 10: tensor([0]), 11: tensor([0]), 12: tensor([]), 13: tensor([]), 14: tensor([]), 15: tensor([0]), 16: tensor([]), 17: tensor([]), 18: tensor([]), 19: tensor([]), 20: tensor([]), 21: tensor([]), 22: tensor([]), 23: tensor([]), 24: tensor([]), 25: tensor([]), 26: tensor([]), 27: tensor([0, 0]), 28: tensor([]), 29: tensor([]), 30: tensor([0, 0, 0]), 31: tensor([]), 32: tensor([]), 33: tensor([]), 34: tensor([]), 35: tensor([]), 36: tensor([]), 37: tensor([]), 38: tensor([]), 39: tensor([0]), 40: tensor([0]), 41: tensor([0, 0]), 42: tensor([0, 0]), 43: tensor([0]), 44: tensor([]), 45: tensor([0, 0, 0, 0, 0]), 46: tensor([0]), 47: tensor([]), 48: tensor([0]), 49: tensor([]), 50: tensor([]), 51: tensor([]), 52: tensor([0]), 53: tensor([0]), 54: tensor([0, 0, 0]), 55: tensor([0]), 56: tensor([0]), 57: tensor([0]), 58: tensor([0, 0]), 59: tensor([]), 60: tensor([]), 61: tensor([]), 62: tensor([0]), 63: tensor([]), 64: tensor([]), 65: tensor([0]), 66: tensor([]), 67: tensor([]), 68: tensor([]), 69: tensor([0, 0]), 70: tensor([0]), 71: tensor([0]), 72: tensor([]), 73: tensor([]), 74: tensor([]), 75: tensor([0]), 76: tensor([]), 77: tensor([0, 0, 0, 0]), 78: tensor([]), 79: tensor([]), 80: tensor([]), 81: tensor([]), 82: tensor([]), 83: tensor([]), 84: tensor([]), 85: tensor([]), 86: tensor([0]), 87: tensor([0]), 88: tensor([]), 89: tensor([0]), 90: tensor([]), 91: tensor([]), 92: tensor([0]), 93: tensor([]), 94: tensor([0]), 95: tensor([]), 96: tensor([0, 0]), 97: tensor([]), 98: tensor([]), 99: tensor([]), 100: tensor([0, 0, 0]), 101: tensor([]), 102: tensor([]), 103: tensor([0, 0, 0]), 104: tensor([]), 105: tensor([0]), 106: tensor([]), 107: tensor([]), 108: tensor([]), 109: tensor([]), 110: tensor([]), 111: tensor([0]), 112: tensor([0]), 113: tensor([]), 114: tensor([]), 115: tensor([0, 0]), 116: tensor([0]), 117: tensor([0]), 118: tensor([]), 119: tensor([0, 0, 0, 0]), 120: tensor([0]), 121: tensor([0]), 122: tensor([]), 123: tensor([0]), 124: tensor([]), 125: tensor([]), 126: tensor([0]), 127: tensor([]), 128: tensor([0, 0, 0]), 129: tensor([]), 130: tensor([0, 0]), 131: tensor([]), 132: tensor([]), 133: tensor([0]), 134: tensor([0]), 135: tensor([]), 136: tensor([]), 137: tensor([0]), 138: tensor([0]), 139: tensor([]), 140: tensor([]), 141: tensor([]), 142: tensor([0]), 143: tensor([0, 0]), 144: tensor([]), 145: tensor([0]), 146: tensor([0]), 147: tensor([0]), 148: tensor([]), 149: tensor([]), 150: tensor([0]), 151: tensor([]), 152: tensor([0, 0]), 153: tensor([]), 154: tensor([0]), 155: tensor([]), 156: tensor([]), 157: tensor([]), 158: tensor([]), 159: tensor([]), 160: tensor([0]), 161: tensor([0]), 162: tensor([]), 163: tensor([]), 164: tensor([]), 165: tensor([]), 166: tensor([0]), 167: tensor([]), 168: tensor([0]), 169: tensor([0, 0]), 170: tensor([]), 171: tensor([]), 172: tensor([0, 0, 0, 0, 0, 0, 0]), 173: tensor([0, 0, 0]), 174: tensor([]), 175: tensor([0]), 176: tensor([]), 177: tensor([]), 178: tensor([]), 179: tensor([]), 180: tensor([]), 181: tensor([]), 182: tensor([0]), 183: tensor([]), 184: tensor([]), 185: tensor([0, 0]), 186: tensor([0, 0]), 187: tensor([0]), 188: tensor([]), 189: tensor([0]), 190: tensor([]), 191: tensor([]), 192: tensor([]), 193: tensor([0]), 194: tensor([]), 195: tensor([]), 196: tensor([]), 197: tensor([]), 198: tensor([]), 199: tensor([]), 200: tensor([0]), 201: tensor([0]), 202: tensor([]), 203: tensor([]), 204: tensor([0, 0]), 205: tensor([]), 206: tensor([]), 207: tensor([]), 208: tensor([]), 209: tensor([0, 0, 0]), 210: tensor([]), 211: tensor([0, 0, 0]), 212: tensor([0]), 213: tensor([]), 214: tensor([0, 0]), 215: tensor([]), 216: tensor([0]), 217: tensor([]), 218: tensor([]), 219: tensor([0, 0, 0, 0, 0]), 220: tensor([0, 0, 0, 0]), 221: tensor([]), 222: tensor([0, 0]), 223: tensor([0]), 224: tensor([0]), 225: tensor([]), 226: tensor([0]), 227: tensor([0]), 228: tensor([]), 229: tensor([]), 230: tensor([]), 231: tensor([0, 0]), 232: tensor([]), 233: tensor([0]), 234: tensor([]), 235: tensor([]), 236: tensor([0]), 237: tensor([]), 238: tensor([0]), 239: tensor([]), 240: tensor([]), 241: tensor([0, 0, 0, 0, 0]), 242: tensor([]), 243: tensor([0]), 244: tensor([]), 245: tensor([]), 246: tensor([]), 247: tensor([]), 248: tensor([]), 249: tensor([]), 250: tensor([0]), 251: tensor([]), 252: tensor([]), 253: tensor([]), 254: tensor([]), 255: tensor([0]), 256: tensor([]), 257: tensor([]), 258: tensor([]), 259: tensor([]), 260: tensor([]), 261: tensor([0]), 262: tensor([0]), 263: tensor([]), 264: tensor([0]), 265: tensor([]), 266: tensor([0]), 267: tensor([0]), 268: tensor([]), 269: tensor([]), 270: tensor([]), 271: tensor([]), 272: tensor([]), 273: tensor([]), 274: tensor([]), 275: tensor([]), 276: tensor([]), 277: tensor([0]), 278: tensor([]), 279: tensor([0]), 280: tensor([]), 281: tensor([]), 282: tensor([]), 283: tensor([]), 284: tensor([]), 285: tensor([0]), 286: tensor([]), 287: tensor([]), 288: tensor([]), 289: tensor([]), 290: tensor([]), 291: tensor([0]), 292: tensor([]), 293: tensor([]), 294: tensor([0]), 295: tensor([]), 296: tensor([]), 297: tensor([]), 298: tensor([]), 299: tensor([])}\n"
     ]
    }
   ],
   "source": [
    "print(all_y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ae96657d6d029a00d2618c001ef8f30df5e1f239aa31eb37bc7c02f265ef478"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
