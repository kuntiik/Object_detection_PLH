{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "allied-hudson",
			"metadata": {},
			"source": [
				"# EfficientDet: Pytorch-lightning (with EfficientNet v2 backbone)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "rental-volunteer",
			"metadata": {},
			"source": [
				"By Chris Hughes"
			]
		},
		{
			"cell_type": "markdown",
			"id": "exposed-specialist",
			"metadata": {},
			"source": [
				"The package versions used are:"
			]
		},
		{
			"cell_type": "markdown",
			"id": "focal-needle",
			"metadata": {},
			"source": [
				"## Loading the data"
			]
		},
		{
			"cell_type": "markdown",
			"id": "lesbian-needle",
			"metadata": {},
			"source": [
				"As an example, we shall use the Kaggle cars object detection dataset found at https://www.kaggle.com/sshikamaru/car-object-detection. As this dataset is quite small, and the test set is unlabelled, for simplicity we shall focus on training and evaluating the model on the training set. Therefore, what this evaluation shows us is whether the model is capable of learning the task.\n",
				"\n",
				"It is assumed that the data has been downloaded and unzipped at the following location (update where necessary):"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "theoretical-roman",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"from pathlib import Path\n",
				"\n",
				"dataset_path = Path('/home.stud/kuntluka/dataset/cars/data')\n",
				"list(dataset_path.iterdir())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "humanitarian-syntax",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"train_data_path = dataset_path/'training_images'"
			]
		},
		{
			"cell_type": "markdown",
			"id": "infinite-gambling",
			"metadata": {},
			"source": [
				"The annotations for this dataset are in the form of a csv file, which associates the image name with the corresponding annotations, we can view the format of this by loading it into a dataframe."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "verbal-insert",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"import pandas as pd\n",
				"\n",
				"df = pd.read_csv(dataset_path/'train_solution_bounding_boxes.csv')"
			]
		},
		{
			"cell_type": "markdown",
			"id": "understood-greene",
			"metadata": {},
			"source": [
				"Here, we can see that each row associates the image filename with a bounding box in pascal VOC format. We only have a single class in this case, which is 'car'"
			]
		},
		{
			"cell_type": "markdown",
			"id": "integrated-operation",
			"metadata": {},
			"source": [
				"## Dataset Adaptor"
			]
		},
		{
			"cell_type": "markdown",
			"id": "recreational-momentum",
			"metadata": {},
			"source": [
				"\n",
				"\n",
				"In order to use this in our model, we must first create a DatasetAdaptor, which will convert the raw dataset format into an image and corresponding annotations to feed into the model.\n",
				"\n",
				"First, define some convenience functions so that we can plot images ang their corresponding bounding boxes"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "pressed-motorcycle",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"import matplotlib.pyplot as plt\n",
				"from matplotlib import patches\n",
				"\n",
				"def get_rectangle_edges_from_pascal_bbox(bbox):\n",
				"    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n",
				"\n",
				"    bottom_left = (xmin_top_left, ymax_bottom_right)\n",
				"    width = xmax_bottom_right - xmin_top_left\n",
				"    height = ymin_top_left - ymax_bottom_right\n",
				"\n",
				"    return bottom_left, width, height\n",
				"\n",
				"def draw_pascal_voc_bboxes(\n",
				"    plot_ax,\n",
				"    bboxes,\n",
				"    get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,\n",
				"):\n",
				"    for bbox in bboxes:\n",
				"        bottom_left, width, height = get_rectangle_corners_fn(bbox)\n",
				"\n",
				"        rect_1 = patches.Rectangle(\n",
				"            bottom_left,\n",
				"            width,\n",
				"            height,\n",
				"            linewidth=4,\n",
				"            edgecolor=\"black\",\n",
				"            fill=False,\n",
				"        )\n",
				"        rect_2 = patches.Rectangle(\n",
				"            bottom_left,\n",
				"            width,\n",
				"            height,\n",
				"            linewidth=2,\n",
				"            edgecolor=\"white\",\n",
				"            fill=False,\n",
				"        )\n",
				"\n",
				"        # Add the patch to the Axes\n",
				"        plot_ax.add_patch(rect_1)\n",
				"        plot_ax.add_patch(rect_2)\n",
				"\n",
				"def show_image(\n",
				"    image, bboxes=None, draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)\n",
				"):\n",
				"    fig, ax = plt.subplots(1, figsize=figsize)\n",
				"    ax.imshow(image)\n",
				"\n",
				"    if bboxes is not None:\n",
				"        draw_bboxes_fn(ax, bboxes)\n",
				"\n",
				"    plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "fixed-silicon",
			"metadata": {},
			"source": [
				"Usually, at this point, we would create a PyTorch Dataset to feed this data into the training loop. However, some of this code, such as normalising the image and transforming the labels into the required format, are not specific to this problem and will need to be applied regardless of which dataset is being used. Therefore, let’s focus for now on creating a CarsDatasetAdaptor class, which will convert the specific raw dataset format into an image and corresponding annotations.  An implementation of this is presented below"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "close-necklace",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"from pathlib import Path\n",
				"\n",
				"import PIL\n",
				"\n",
				"import numpy as np\n",
				"\n",
				"class CarsDatasetAdaptor:\n",
				"    def __init__(self, images_dir_path, annotations_dataframe):\n",
				"        self.images_dir_path = Path(images_dir_path)\n",
				"        self.annotations_df = annotations_dataframe\n",
				"        self.images = self.annotations_df.image.unique().tolist()\n",
				"\n",
				"    def __len__(self) -> int:\n",
				"        return len(self.images)\n",
				"\n",
				"    def get_image_and_labels_by_idx(self, index):\n",
				"        image_name = self.images[index]\n",
				"        image = PIL.Image.open(self.images_dir_path / image_name)\n",
				"        pascal_bboxes = self.annotations_df[self.annotations_df.image == image_name][\n",
				"            [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
				"        ].values\n",
				"        class_labels = np.ones(len(pascal_bboxes))\n",
				"\n",
				"        return image, pascal_bboxes, class_labels, index\n",
				"    \n",
				"    def show_image(self, index):\n",
				"        image, bboxes, class_labels, image_id = self.get_image_and_labels_by_idx(index)\n",
				"        print(f\"image_id: {image_id}\")\n",
				"        show_image(image, bboxes.tolist())\n",
				"        print(class_labels)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "precise-tonight",
			"metadata": {},
			"source": [
				"the get_image_and_labels_by_idx method returns a tuple containing:\n",
				"\n",
				"- image: A PIL image\n",
				"- pascal_bboxes: a numpy array of shape [N, 4] containing the ground truth bounding boxes in Pascal VOC format\n",
				"- class_labels: a numpy array of shape N containing the ground truth class labels\n",
				"- image_id : a unique identifier which can be used to identify the image\n",
				"and the __len__ method.\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "interested-baltimore",
			"metadata": {},
			"source": [
				"As we can see, in this case, this class simply wraps the dataframe provided with the dataset. We can now create an instance of this class to provide a clean interface to view the training data. As this dataset only contains a single class, ones are always returned in this case. Additionally, as the image_id can be any unique identifer associated with the image, here we have just used the index of the image in the dataset."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "simple-outreach",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"cars_train_ds = CarsDatasetAdaptor(train_data_path, df)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "indie-joining",
			"metadata": {},
			"source": [
				"## Creating the model"
			]
		},
		{
			"cell_type": "markdown",
			"id": "earned-worthy",
			"metadata": {},
			"source": [
				"There are several architecture variants available for EfficientDet, so we can view these as follows."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "canadian-quarterly",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"# from effdet.config.model_config import efficientdet_model_param_dict\n",
				"from effdet.config.model_config import efficientdet_model_param_dict\n",
				"from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
				"from effdet.efficientdet import HeadNet\n",
				"from effdet.config.model_config import efficientdet_model_param_dict"
			]
		},
		{
			"cell_type": "markdown",
			"id": "recreational-brave",
			"metadata": {},
			"source": [
				"Now, let’s look at creating the EfficientDet model. Thanks to Ross Wightman’s effdet and timm libraries, we have many options here. The effdet package includes a selection of different EfficientDet configurations which can be used. We can view a selection of these below."
			]
		},
		{
			"cell_type": "markdown",
			"id": "heated-anniversary",
			"metadata": {},
			"source": [
				"Some of these implementations (i.e. efficientdet_d5) have been trained by Ross in PyTorch, whereas any implementation prefixed by ‘tf_’ uses the official pretrained weights. As the initial models were trained in TensorFlow, to use these weights in PyTorch, certain modifications have been made (such as implementing ‘same’ padding) which means that these models may be slower during training and inference."
			]
		},
		{
			"cell_type": "markdown",
			"id": "lesser-vulnerability",
			"metadata": {},
			"source": [
				"In addition to the provided configs, we can also use any model from timm as our EfficientDet backbone. Here, let’s try using one of the new EfficientNetv2 models as the backbone. Similarly to before, we can list these models using timm:\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "comfortable-helping",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"import timm"
			]
		},
		{
			"cell_type": "markdown",
			"id": "wrong-boston",
			"metadata": {},
			"source": [
				"To use one of these models, we first must register it as an EfficientDet config by adding a dictionary to the `efficientdet_model_param_dict`. Let’s create a function that does this for us, and then creates the EfficientDet model using the machinery from effdet:"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "restricted-insertion",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"def create_model(num_classes=1, image_size=512, architecture=\"tf_efficientnetv2_l\"):\n",
				"    efficientdet_model_param_dict[architecture] = dict(\n",
				"        name=architecture,\n",
				"        backbone_name=architecture,\n",
				"        backbone_args=dict(drop_path_rate=0.2),\n",
				"        num_classes=num_classes,\n",
				"        url='', )\n",
				"    \n",
				"    config = get_efficientdet_config(architecture)\n",
				"    config.update({'num_classes': num_classes})\n",
				"    config.update({'image_size': (image_size, image_size)})\n",
				"    \n",
				"    print(config)\n",
				"\n",
				"    net = EfficientDet(config, pretrained_backbone=True)\n",
				"    net.class_net = HeadNet(\n",
				"        config,\n",
				"        num_outputs=config.num_classes,\n",
				"    )\n",
				"    return DetBenchTrain(net, config)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "c1b00e30",
			"metadata": {},
			"outputs": [],
			"source": [
				"model = create_model(architecture='tf_efficientnetv2_m')"
			]
		},
		{
			"cell_type": "markdown",
			"id": "subjective-outreach",
			"metadata": {},
			"source": [
				"We can now create an instance of the model. The model has parameters such as num_classes which should be set based on the specific problem.\n",
				"\n",
				"Due to the architecture of EfficientDet, the input image size must be divisible by 128, see https://medium.com/@nainaakash012/efficientdet-scalable-and-efficient-object-detection-ea05ccd28427 for more details. Here, we use the default size of 512. Note that, when altering this, you must also alter the default transfroms which are used by the model, by passing new functions as parameters. This is easy to do, and will become straightforward after inspecting the source code."
			]
		},
		{
			"cell_type": "markdown",
			"id": "english-clinic",
			"metadata": {},
			"source": [
				"## Define the EfficientDet Dataset"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "recovered-childhood",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"from torch.utils.data import Dataset\n",
				"\n",
				"import albumentations as A\n",
				"from albumentations.pytorch.transforms import ToTensorV2\n",
				"\n",
				"\n",
				"def get_train_transforms(target_img_size=512):\n",
				"    return A.Compose(\n",
				"        [\n",
				"            A.HorizontalFlip(p=0.5),\n",
				"            A.Resize(height=target_img_size, width=target_img_size, p=1),\n",
				"            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
				"            ToTensorV2(p=1),\n",
				"        ],\n",
				"        p=1.0,\n",
				"        bbox_params=A.BboxParams(\n",
				"            format=\"pascal_voc\", min_area=0, min_visibility=0, label_fields=[\"labels\"]\n",
				"        ),\n",
				"    )\n",
				"\n",
				"\n",
				"def get_valid_transforms(target_img_size=512):\n",
				"    return A.Compose(\n",
				"        [\n",
				"            A.Resize(height=target_img_size, width=target_img_size, p=1),\n",
				"            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
				"            ToTensorV2(p=1),\n",
				"        ],\n",
				"        p=1.0,\n",
				"        bbox_params=A.BboxParams(\n",
				"            format=\"pascal_voc\", min_area=0, min_visibility=0, label_fields=[\"labels\"]\n",
				"        ),\n",
				"    )\n",
				"\n",
				"class EfficientDetDataset(Dataset):\n",
				"    def __init__(\n",
				"        self, dataset_adaptor, transforms=get_valid_transforms()\n",
				"    ):\n",
				"        self.ds = dataset_adaptor\n",
				"        self.transforms = transforms\n",
				"\n",
				"    def __getitem__(self, index):\n",
				"        (\n",
				"            image,\n",
				"            pascal_bboxes,\n",
				"            class_labels,\n",
				"            image_id,\n",
				"        ) = self.ds.get_image_and_labels_by_idx(index)\n",
				"\n",
				"        sample = {\n",
				"            \"image\": np.array(image, dtype=np.float32),\n",
				"            \"bboxes\": pascal_bboxes,\n",
				"            \"labels\": class_labels,\n",
				"        }\n",
				"\n",
				"        sample = self.transforms(**sample)\n",
				"        sample[\"bboxes\"] = np.array(sample[\"bboxes\"])\n",
				"        image = sample[\"image\"]\n",
				"        pascal_bboxes = sample[\"bboxes\"]\n",
				"        labels = sample[\"labels\"]\n",
				"\n",
				"        _, new_h, new_w = image.shape\n",
				"        sample[\"bboxes\"][:, [0, 1, 2, 3]] = sample[\"bboxes\"][\n",
				"            :, [1, 0, 3, 2]\n",
				"        ]  # convert to yxyx\n",
				"\n",
				"        target = {\n",
				"            \"bboxes\": torch.as_tensor(sample[\"bboxes\"], dtype=torch.float32),\n",
				"            \"labels\": torch.as_tensor(labels),\n",
				"            \"image_id\": torch.tensor([image_id]),\n",
				"            \"img_size\": (new_h, new_w),\n",
				"            \"img_scale\": torch.tensor([1.0]),\n",
				"        }\n",
				"\n",
				"        return image, target, image_id\n",
				"\n",
				"    def __len__(self):\n",
				"        return len(self.ds)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "spatial-exhibition",
			"metadata": {},
			"source": [
				"## Define the DataModule"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "powerful-screw",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"from pytorch_lightning import LightningDataModule\n",
				"from torch.utils.data import DataLoader\n",
				"\n",
				"class EfficientDetDataModule(LightningDataModule):\n",
				"    \n",
				"    def __init__(self,\n",
				"                train_dataset_adaptor,\n",
				"                validation_dataset_adaptor,\n",
				"                train_transforms=get_train_transforms(target_img_size=512),\n",
				"                valid_transforms=get_valid_transforms(target_img_size=512),\n",
				"                num_workers=4,\n",
				"                batch_size=8):\n",
				"        \n",
				"        self.train_ds = train_dataset_adaptor\n",
				"        self.valid_ds = validation_dataset_adaptor\n",
				"        self.train_tfms = train_transforms\n",
				"        self.valid_tfms = valid_transforms\n",
				"        self.num_workers = num_workers\n",
				"        self.batch_size = batch_size\n",
				"        super().__init__()\n",
				"\n",
				"    def train_dataset(self) -> EfficientDetDataset:\n",
				"        return EfficientDetDataset(\n",
				"            dataset_adaptor=self.train_ds, transforms=self.train_tfms\n",
				"        )\n",
				"\n",
				"    def train_dataloader(self) -> DataLoader:\n",
				"        train_dataset = self.train_dataset()\n",
				"        train_loader = torch.utils.data.DataLoader(\n",
				"            train_dataset,\n",
				"            batch_size=self.batch_size,\n",
				"            shuffle=False,\n",
				"            pin_memory=True,\n",
				"            drop_last=True,\n",
				"            num_workers=self.num_workers,\n",
				"            collate_fn=self.collate_fn,\n",
				"        )\n",
				"\n",
				"        return train_loader\n",
				"\n",
				"    def val_dataset(self) -> EfficientDetDataset:\n",
				"        return EfficientDetDataset(\n",
				"            dataset_adaptor=self.valid_ds, transforms=self.valid_tfms\n",
				"        )\n",
				"\n",
				"    def val_dataloader(self) -> DataLoader:\n",
				"        valid_dataset = self.val_dataset()\n",
				"        valid_loader = torch.utils.data.DataLoader(\n",
				"            valid_dataset,\n",
				"            batch_size=self.batch_size,\n",
				"            shuffle=False,\n",
				"            pin_memory=True,\n",
				"            drop_last=True,\n",
				"            num_workers=self.num_workers,\n",
				"            collate_fn=self.collate_fn,\n",
				"        )\n",
				"\n",
				"        return valid_loader\n",
				"    \n",
				"    @staticmethod\n",
				"    def collate_fn(batch):\n",
				"        images, targets, image_ids = tuple(zip(*batch))\n",
				"        images = torch.stack(images)\n",
				"        images = images.float()\n",
				"\n",
				"        boxes = [target[\"bboxes\"].float() for target in targets]\n",
				"        labels = [target[\"labels\"].float() for target in targets]\n",
				"        img_size = torch.tensor([target[\"img_size\"] for target in targets]).float()\n",
				"        img_scale = torch.tensor([target[\"img_scale\"] for target in targets]).float()\n",
				"\n",
				"        annotations = {\n",
				"            \"bbox\": boxes,\n",
				"            \"cls\": labels,\n",
				"            \"img_size\": img_size,\n",
				"            \"img_scale\": img_scale,\n",
				"        }\n",
				"\n",
				"        return images, annotations"
			]
		},
		{
			"cell_type": "markdown",
			"id": "upper-porcelain",
			"metadata": {},
			"source": [
				"## Define the training loop"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "sixth-ceramic",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"from numbers import Number\n",
				"from typing import List\n",
				"from functools import singledispatch\n",
				"\n",
				"import numpy as np\n",
				"import torch\n",
				"\n",
				"# from fastcore.dispatch import typedispatch\n",
				"from pytorch_lightning import LightningModule\n",
				"# from pytorch_lightning.core.decorators import auto_move_data\n",
				"\n",
				"\n",
				"# from ensemble_boxes import ensemble_boxes_wbf\n",
				"\n",
				"\n",
				"# def run_wbf(predictions, image_size=512, iou_thr=0.44, skip_box_thr=0.43, weights=None):\n",
				"#     bboxes = []\n",
				"#     confidences = []\n",
				"#     class_labels = []\n",
				"\n",
				"#     for prediction in predictions:\n",
				"#         boxes = [(prediction[\"boxes\"] / image_size).tolist()]\n",
				"#         scores = [prediction[\"scores\"].tolist()]\n",
				"#         labels = [prediction[\"classes\"].tolist()]\n",
				"\n",
				"#         boxes, scores, labels = ensemble_boxes_wbf.weighted_boxes_fusion(\n",
				"#             boxes,\n",
				"#             scores,\n",
				"#             labels,\n",
				"#             weights=weights,\n",
				"#             iou_thr=iou_thr,\n",
				"#             skip_box_thr=skip_box_thr,\n",
				"#         )\n",
				"#         boxes = boxes * (image_size - 1)\n",
				"#         bboxes.append(boxes.tolist())\n",
				"#         confidences.append(scores.tolist())\n",
				"#         class_labels.append(labels.tolist())\n",
				"\n",
				"#     return bboxes, confidences, class_labels\n",
				"\n",
				"\n",
				"class EfficientDetModel(LightningModule):\n",
				"    def __init__(\n",
				"        self,\n",
				"        num_classes=1,\n",
				"        img_size=512,\n",
				"        prediction_confidence_threshold=0.2,\n",
				"        learning_rate=0.0002,\n",
				"        wbf_iou_threshold=0.44,\n",
				"        inference_transforms=get_valid_transforms(target_img_size=512),\n",
				"        model_architecture='tf_efficientnetv2_l',\n",
				"    ):\n",
				"        super().__init__()\n",
				"        self.img_size = img_size\n",
				"        self.model = create_model(\n",
				"            num_classes, img_size, architecture=model_architecture\n",
				"        )\n",
				"        self.prediction_confidence_threshold = prediction_confidence_threshold\n",
				"        self.lr = learning_rate\n",
				"        self.wbf_iou_threshold = wbf_iou_threshold\n",
				"        self.inference_tfms = inference_transforms\n",
				"\n",
				"\n",
				"    # @auto_move_data\n",
				"    def forward(self, images, targets):\n",
				"        return self.model(images, targets)\n",
				"\n",
				"    def configure_optimizers(self):\n",
				"        return torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
				"\n",
				"\n",
				"    def training_step(self, batch, batch_idx):\n",
				"        images, annotations = batch\n",
				"\n",
				"        losses = self.model(images, annotations)\n",
				"\n",
				"        logging_losses = {\n",
				"            \"class_loss\": losses[\"class_loss\"].detach(),\n",
				"            \"box_loss\": losses[\"box_loss\"].detach(),\n",
				"        }\n",
				"\n",
				"        self.log(\"train_loss\", losses[\"loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
				"                 logger=True)\n",
				"        self.log(\n",
				"            \"train_class_loss\", losses[\"class_loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
				"            logger=True\n",
				"        )\n",
				"        self.log(\"train_box_loss\", losses[\"box_loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
				"                 logger=True)\n",
				"\n",
				"        return losses['loss']\n",
				"\n",
				"\n",
				"    @torch.no_grad()\n",
				"    def validation_step(self, batch, batch_idx):\n",
				"        images, annotations = batch\n",
				"        outputs = self.model(images, annotations)\n",
				"\n",
				"        detections = outputs[\"detections\"]\n",
				"\n",
				"\n",
				"        logging_losses = {\n",
				"            \"class_loss\": outputs[\"class_loss\"].detach(),\n",
				"            \"box_loss\": outputs[\"box_loss\"].detach(),\n",
				"        }\n",
				"\n",
				"        self.log(\"valid_loss\", outputs[\"loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
				"                 logger=True, sync_dist=True)\n",
				"        self.log(\n",
				"            \"valid_class_loss\", logging_losses[\"class_loss\"], on_step=True, on_epoch=True,\n",
				"            prog_bar=True, logger=True, sync_dist=True\n",
				"        )\n",
				"        self.log(\"valid_box_loss\", logging_losses[\"box_loss\"], on_step=True, on_epoch=True,\n",
				"                 prog_bar=True, logger=True, sync_dist=True)\n",
				"\n",
				"        return {'loss': outputs[\"loss\"]}\n",
				"    \n",
				"    \n",
				"    # @typedispatch\n",
				"    # def predict(self, images: List):\n",
				"    #     \"\"\"\n",
				"    #     For making predictions from images\n",
				"    #     Args:\n",
				"    #         images: a list of PIL images\n",
				"\n",
				"    #     Returns: a tuple of lists containing bboxes, predicted_class_labels, predicted_class_confidences\n",
				"\n",
				"    #     \"\"\"\n",
				"    #     image_sizes = [(image.size[1], image.size[0]) for image in images]\n",
				"    #     images_tensor = torch.stack(\n",
				"    #         [\n",
				"    #             self.inference_tfms(\n",
				"    #                 image=np.array(image, dtype=np.float32),\n",
				"    #                 labels=np.ones(1),\n",
				"    #                 bboxes=np.array([[0, 0, 1, 1]]),\n",
				"    #             )[\"image\"]\n",
				"    #             for image in images\n",
				"    #         ]\n",
				"    #     )\n",
				"\n",
				"        return self._run_inference(images_tensor, image_sizes)\n",
				"\n",
				"    # @typedispatch\n",
				"    def predict(self, images_tensor: torch.Tensor):\n",
				"        \"\"\"\n",
				"        For making predictions from tensors returned from the model's dataloader\n",
				"        Args:\n",
				"            images_tensor: the images tensor returned from the dataloader\n",
				"\n",
				"        Returns: a tuple of lists containing bboxes, predicted_class_labels, predicted_class_confidences\n",
				"\n",
				"        \"\"\"\n",
				"        if images_tensor.ndim == 3:\n",
				"            images_tensor = images_tensor.unsqueeze(0)\n",
				"        if (\n",
				"            images_tensor.shape[-1] != self.img_size\n",
				"            or images_tensor.shape[-2] != self.img_size\n",
				"        ):\n",
				"            raise ValueError(\n",
				"                f\"Input tensors must be of shape (N, 3, {self.img_size}, {self.img_size})\"\n",
				"            )\n",
				"\n",
				"        num_images = images_tensor.shape[0]\n",
				"        image_sizes = [(self.img_size, self.img_size)] * num_images\n",
				"\n",
				"        return self._run_inference(images_tensor, image_sizes)\n",
				"\n",
				"    # def _run_inference(self, images_tensor, image_sizes):\n",
				"    #     dummy_targets = self._create_dummy_inference_targets(\n",
				"    #         num_images=images_tensor.shape[0]\n",
				"    #     )\n",
				"\n",
				"    #     detections = self.model(images_tensor.to(self.device), dummy_targets)[\n",
				"    #         \"detections\"\n",
				"    #     ]\n",
				"    #     (\n",
				"    #         predicted_bboxes,\n",
				"    #         predicted_class_confidences,\n",
				"    #         predicted_class_labels,\n",
				"    #     ) = self.post_process_detections(detections)\n",
				"\n",
				"    #     scaled_bboxes = self.__rescale_bboxes(\n",
				"    #         predicted_bboxes=predicted_bboxes, image_sizes=image_sizes\n",
				"    #     )\n",
				"\n",
				"    #     return scaled_bboxes, predicted_class_labels, predicted_class_confidences\n",
				"    \n",
				"    # def _create_dummy_inference_targets(self, num_images):\n",
				"    #     dummy_targets = {\n",
				"    #         \"bbox\": [\n",
				"    #             torch.tensor([[0.0, 0.0, 0.0, 0.0]], device=self.device)\n",
				"    #             for i in range(num_images)\n",
				"    #         ],\n",
				"    #         \"cls\": [torch.tensor([1.0], device=self.device) for i in range(num_images)],\n",
				"    #         \"img_size\": torch.tensor(\n",
				"    #             [(self.img_size, self.img_size)] * num_images, device=self.device\n",
				"    #         ).float(),\n",
				"    #         \"img_scale\": torch.ones(num_images, device=self.device).float(),\n",
				"    #     }\n",
				"\n",
				"    #     return dummy_targets\n",
				"    \n",
				"    # def post_process_detections(self, detections):\n",
				"    #     predictions = []\n",
				"    #     for i in range(detections.shape[0]):\n",
				"    #         predictions.append(\n",
				"    #             self._postprocess_single_prediction_detections(detections[i])\n",
				"    #         )\n",
				"\n",
				"    #     predicted_bboxes, predicted_class_confidences, predicted_class_labels = run_wbf(\n",
				"    #         predictions, image_size=self.img_size, iou_thr=self.wbf_iou_threshold\n",
				"    #     )\n",
				"\n",
				"    #     return predicted_bboxes, predicted_class_confidences, predicted_class_labels\n",
				"\n",
				"    # def _postprocess_single_prediction_detections(self, detections):\n",
				"    #     boxes = detections.detach().cpu().numpy()[:, :4]\n",
				"    #     scores = detections.detach().cpu().numpy()[:, 4]\n",
				"    #     classes = detections.detach().cpu().numpy()[:, 5]\n",
				"    #     indexes = np.where(scores > self.prediction_confidence_threshold)[0]\n",
				"    #     boxes = boxes[indexes]\n",
				"\n",
				"    #     return {\"boxes\": boxes, \"scores\": scores[indexes], \"classes\": classes[indexes]}\n",
				"\n",
				"    # def __rescale_bboxes(self, predicted_bboxes, image_sizes):\n",
				"    #     scaled_bboxes = []\n",
				"    #     for bboxes, img_dims in zip(predicted_bboxes, image_sizes):\n",
				"    #         im_h, im_w = img_dims\n",
				"\n",
				"    #         if len(bboxes) > 0:\n",
				"    #             scaled_bboxes.append(\n",
				"    #                 (\n",
				"    #                     np.array(bboxes)\n",
				"    #                     * [\n",
				"    #                         im_w / self.img_size,\n",
				"    #                         im_h / self.img_size,\n",
				"    #                         im_w / self.img_size,\n",
				"    #                         im_h / self.img_size,\n",
				"    #                     ]\n",
				"    #                 ).tolist()\n",
				"    #             )\n",
				"    #         else:\n",
				"    #             scaled_bboxes.append(bboxes)\n",
				"\n",
				"    #     return scaled_bboxes\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "mathematical-temple",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"dm = EfficientDetDataModule(train_dataset_adaptor=cars_train_ds, \n",
				"        validation_dataset_adaptor=cars_train_ds,\n",
				"        num_workers=4,\n",
				"        batch_size=64)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "09775a67",
			"metadata": {},
			"outputs": [],
			"source": [
				"import sys\n",
				"sys.path.append(\"../\")\n",
				"from src.datamodules.cars_datamodule import CarsDataModule\n",
				"# from src.datamodules."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "canadian-feeling",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"model = EfficientDetModel(\n",
				"    num_classes=1,\n",
				"    img_size=512\n",
				"    )"
			]
		},
		{
			"cell_type": "markdown",
			"id": "standing-discipline",
			"metadata": {},
			"source": [
				"As the EfficientDet model is just a standard PyTorch Lightning model, it can be trained in the usual way, by importing and creating an appropriate trainer."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "martial-rings",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"from pytorch_lightning import Trainer"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "bf9aacb3",
			"metadata": {},
			"outputs": [],
			"source": [
				"dm1 = CarsDataModule(batch_size=64, data_split = [1001, 0 , 0])\n",
				"dm1.setup()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "3d9c384a",
			"metadata": {},
			"outputs": [],
			"source": [
				"l = dm.train_dataloader()\n",
				"l1 = dm1.train_dataloader()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "9c01770e",
			"metadata": {},
			"outputs": [],
			"source": [
				"b = next(iter(l))\n",
				"b1 = next(iter(l1))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "71d0f185",
			"metadata": {},
			"outputs": [],
			"source": [
				"b"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "c1571a5e",
			"metadata": {},
			"outputs": [],
			"source": [
				"from torchvision.utils import make_grid\n",
				"import matplotlib.pyplot as plt"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "5175cfed",
			"metadata": {},
			"outputs": [],
			"source": [
				"grid = make_grid(b[0], nrow=8)\n",
				"plt.figure(figsize = (40, 30))\n",
				"plt.imshow(grid.permute(1,2,0))\n",
				"plt.show()\n",
				"# b[1]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f7a518c9",
			"metadata": {},
			"outputs": [],
			"source": [
				"grid = make_grid(b1[0], nrow=8)\n",
				"plt.figure(figsize = (40, 30))\n",
				"plt.imshow(grid.permute(1,2,0))\n",
				"plt.show()\n",
				"# b1[1]\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "published-noise",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"trainer = Trainer(\n",
				"        gpus=[0], max_epochs=5, num_sanity_val_steps=1, overfit_batches = 4\n",
				"    )"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "vocational-render",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"trainer.fit(model, dm)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "individual-footage",
			"metadata": {},
			"source": [
				"We can save this like a regular PyTorch model"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "framed-moisture",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"torch.save(model.state_dict(), 'trained_effdet')"
			]
		},
		{
			"cell_type": "markdown",
			"id": "supposed-denver",
			"metadata": {},
			"source": [
				"We can load our trained model again as follows"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "uniform-iceland",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"model = EfficientDetModel(\n",
				"    num_classes=1,\n",
				"    img_size=512\n",
				"    )\n",
				"\n",
				"model.load_state_dict(torch.load('trained_effdet'))"
			]
		},
		{
			"cell_type": "markdown",
			"id": "coral-flesh",
			"metadata": {},
			"source": [
				"## Using the model for inference"
			]
		},
		{
			"cell_type": "markdown",
			"id": "bibliographic-struggle",
			"metadata": {},
			"source": [
				"Now we have finetuned the model on our dataset, we can inspect some of the predictions. First we put the model into eval mode."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "objective-catering",
			"metadata": {
				"scrolled": true,
				"trusted": false
			},
			"outputs": [],
			"source": [
				"model.eval()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "quantitative-custom",
			"metadata": {},
			"source": [
				"We can now use our dataset adaptor to load a selection of images"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "willing-wisconsin",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"image1, truth_bboxes1, _, _ = cars_train_ds.get_image_and_labels_by_idx(0)\n",
				"image2, truth_bboxes2, _, _ = cars_train_ds.get_image_and_labels_by_idx(1)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "white-password",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"images = [image1, image2]"
			]
		},
		{
			"cell_type": "markdown",
			"id": "drawn-quarter",
			"metadata": {},
			"source": [
				"and the model's predict function to get the predicted bounding boxes for these images"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "celtic-hearts",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"predicted_bboxes, predicted_class_confidences, predicted_class_labels = model.predict(images)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "attractive-leisure",
			"metadata": {},
			"source": [
				"We can visualise these predictions using a convenience function"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "ordered-immune",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"def compare_bboxes_for_image(\n",
				"    image,\n",
				"    predicted_bboxes,\n",
				"    actual_bboxes,\n",
				"    draw_bboxes_fn=draw_pascal_voc_bboxes,\n",
				"    figsize=(20, 20),\n",
				"):\n",
				"    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
				"    ax1.imshow(image)\n",
				"    ax1.set_title(\"Prediction\")\n",
				"    ax2.imshow(image)\n",
				"    ax2.set_title(\"Actual\")\n",
				"\n",
				"    draw_bboxes_fn(ax1, predicted_bboxes)\n",
				"    draw_bboxes_fn(ax2, actual_bboxes)\n",
				"\n",
				"    plt.show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "nonprofit-mitchell",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"compare_bboxes_for_image(image1, predicted_bboxes=predicted_bboxes[0], actual_bboxes=truth_bboxes1.tolist())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "occasional-layout",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"compare_bboxes_for_image(image2, predicted_bboxes=predicted_bboxes[1], actual_bboxes=truth_bboxes2.tolist())"
			]
		},
		{
			"cell_type": "markdown",
			"id": "sensitive-kernel",
			"metadata": {},
			"source": [
				"## Using model hooks to manually debug"
			]
		},
		{
			"cell_type": "markdown",
			"id": "approximate-option",
			"metadata": {},
			"source": [
				"One feature of PyTorch lightning is that it uses methods or 'hooks' to represent each part of the training process. Whilst we lose some visibility over our training loop when using the Trainer, we can use these hooks to easily debug each step.\n",
				"\n",
				"For example, we can use a hook defined on our DataModule to get the dataloader that will be used during validation and use this to grab a batch."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "later-denial",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"loader = dm.val_dataloader()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "measured-manhattan",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"dl_iter = iter(loader)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "engaging-boring",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"batch = next(dl_iter)\n",
				"batch[1]"
			]
		},
		{
			"cell_type": "markdown",
			"id": "endangered-priority",
			"metadata": {},
			"source": [
				"### Validation outputs and Coco metrics"
			]
		},
		{
			"cell_type": "markdown",
			"id": "muslim-builder",
			"metadata": {},
			"source": [
				"We can use this batch to see exactly what the model calculated during validation. As lightning takes care of moving data to the correct device during training, for simplicity, we shall do this on the cpu so that we don't have to manually move all of the tensors in each batch to the device."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "thrown-match",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"device = model.device; device"
			]
		},
		{
			"cell_type": "markdown",
			"id": "respected-office",
			"metadata": {},
			"source": [
				"Using the model's hook, we can see what is calculated for each batch during each validation step"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "hybrid-approval",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"output = model.validation_step(batch=batch, batch_idx=0)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "bored-feeling",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"output"
			]
		},
		{
			"cell_type": "markdown",
			"id": "sacred-fellow",
			"metadata": {},
			"source": [
				"Here, we can see that the loss is returned for the batch, as well as the predictions and targets.\n",
				"\n",
				"In order to calculate metrics, for the epoch, we need to get the predictions corresponding to each batch. As the `validation_step` method will be called for each batch, let's define a function to aggregate the outputs. \n",
				"\n",
				"Here, for simplicity, we will patch this function to the EfficientDet class using a convenience decorator from fastcore - we pay a huge performance price for Python being a dynamic language, we may as well make the most of it!"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "latin-issue",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"from fastcore.basics import patch\n",
				"\n",
				"@patch\n",
				"def aggregate_prediction_outputs(self: EfficientDetModel, outputs):\n",
				"\n",
				"    detections = torch.cat(\n",
				"        [output[\"batch_predictions\"][\"predictions\"] for output in outputs]\n",
				"    )\n",
				"\n",
				"    image_ids = []\n",
				"    targets = []\n",
				"    for output in outputs:\n",
				"        batch_predictions = output[\"batch_predictions\"]\n",
				"        image_ids.extend(batch_predictions[\"image_ids\"])\n",
				"        targets.extend(batch_predictions[\"targets\"])\n",
				"\n",
				"    (\n",
				"        predicted_bboxes,\n",
				"        predicted_class_confidences,\n",
				"        predicted_class_labels,\n",
				"    ) = self.post_process_detections(detections)\n",
				"\n",
				"    return (\n",
				"        predicted_class_labels,\n",
				"        image_ids,\n",
				"        predicted_bboxes,\n",
				"        predicted_class_confidences,\n",
				"        targets,\n",
				"    )"
			]
		},
		{
			"cell_type": "markdown",
			"id": "accessory-marketplace",
			"metadata": {},
			"source": [
				"From the PyTorch-lightning docs (see https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#validation-epoch-level-metrics), we can see that we can add an additional hook `validation_epoch_end` which is called after all batches have been processed; at the end of each epoch, a list of step outputs are passed to this hook.\n",
				"\n",
				"Let's use this hook to calculate the overall validation loss, as well as the COCO metrics using the `objdetecteval` package. we can use the output that we just calculated when evaluating a single validation batch, but this approach would also extend to the validation loop evaluation during training with lightning."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "higher-climb",
			"metadata": {
				"scrolled": true,
				"trusted": false
			},
			"outputs": [],
			"source": [
				"!pip install git+https://github.com/alexhock/object-detection-metrics"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "outside-album",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"from objdetecteval.metrics.coco_metrics import get_coco_stats"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "introductory-target",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"@patch\n",
				"def validation_epoch_end(self: EfficientDetModel, outputs):\n",
				"    \"\"\"Compute and log training loss and accuracy at the epoch level.\"\"\"\n",
				"\n",
				"    validation_loss_mean = torch.stack(\n",
				"        [output[\"loss\"] for output in outputs]\n",
				"    ).mean()\n",
				"\n",
				"    (\n",
				"        predicted_class_labels,\n",
				"        image_ids,\n",
				"        predicted_bboxes,\n",
				"        predicted_class_confidences,\n",
				"        targets,\n",
				"    ) = self.aggregate_prediction_outputs(outputs)\n",
				"\n",
				"    truth_image_ids = [target[\"image_id\"].detach().item() for target in targets]\n",
				"    truth_boxes = [\n",
				"        target[\"bboxes\"].detach()[:, [1, 0, 3, 2]].tolist() for target in targets\n",
				"    ] # convert to xyxy for evaluation\n",
				"    truth_labels = [target[\"labels\"].detach().tolist() for target in targets]\n",
				"\n",
				"    stats = get_coco_stats(\n",
				"        prediction_image_ids=image_ids,\n",
				"        predicted_class_confidences=predicted_class_confidences,\n",
				"        predicted_bboxes=predicted_bboxes,\n",
				"        predicted_class_labels=predicted_class_labels,\n",
				"        target_image_ids=truth_image_ids,\n",
				"        target_bboxes=truth_boxes,\n",
				"        target_class_labels=truth_labels,\n",
				"    )['All']\n",
				"\n",
				"    return {\"val_loss\": validation_loss_mean, \"metrics\": stats}"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "consolidated-substance",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"model.validation_epoch_end([output])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "accepting-courtesy",
			"metadata": {},
			"source": [
				"### For inference"
			]
		},
		{
			"cell_type": "markdown",
			"id": "meaningful-choice",
			"metadata": {},
			"source": [
				"We can also use the predict function directly on the processed images returned from our data loader. Let's  now unpack the batch to just get the images, as we don't need the labels for inference."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "likely-payment",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"images, annotations,targets, image_ids = batch"
			]
		},
		{
			"cell_type": "markdown",
			"id": "powered-patient",
			"metadata": {},
			"source": [
				"Thanks to the `typedispatch` decorator, we can use the same predict function signature on these tensors."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "executive-treatment",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"predicted_bboxes, predicted_class_labels, predicted_class_confidences = model.predict(images)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "bored-chick",
			"metadata": {},
			"source": [
				"It is important to note at this point that the images given by the dataloader have already been transformed and scaled to size 512. Therefore, the bounding boxes predicted will be relative for an image of 512. As such, to visualise these predictions on the original image, we must rescale it."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "ahead-space",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"image, _, _, _ = cars_train_ds.get_image_and_labels_by_idx(0)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "textile-medline",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": [
				"show_image(image.resize((512, 512)), predicted_bboxes[0])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "introductory-matthew",
			"metadata": {},
			"source": [
				"As we can see, after rescaling, the bounding box is in the correct position!"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "administrative-swedish",
			"metadata": {
				"trusted": false
			},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"gist": {
			"data": {
				"description": "EfficientDet Pytorch-lightning with EfficientNet v2 backbone Blog Post.ipynb",
				"public": false
			},
			"id": ""
		},
		"interpreter": {
			"hash": "013944104d3649cf523d6fa94b2fe35daed4a9a079f2ef1fffaad103ab316349"
		},
		"kernelspec": {
			"display_name": "effdet-blog",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.8.12"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
